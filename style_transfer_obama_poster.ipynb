{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer_obama_poster.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamithi/learnpython/blob/master/style_transfer_obama_poster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcunoHV-3bev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oWgEQNU9UeZ",
        "colab_type": "code",
        "outputId": "eb0b1637-d3c3-4de7-ff6c-8c27302c8010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "\n",
        "import torch \n",
        "import torch.optim as optim \n",
        "import requests \n",
        "from torchvision import transforms, models \n",
        "\n",
        "# Get the \"features\" portion of VGG19\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# Freeze all VGG parameters since we're only optimizing the target image\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vgg.to(device)\n",
        "\n",
        "def load_image(img_path, max_size=400, shape=None):\n",
        "    # Load in and transform an image\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "    else:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    in_transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "        (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    # Discard the transparent, alpha channel (that's teh :3) and add teh batch dimension\n",
        "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
        "\n",
        "    return image\n",
        "\n",
        "# load in content and style image\n",
        "content = load_image('tiger.jpeg').to(device)\n",
        "# Resize style to match content, makes code easier\n",
        "style = load_image('fire.jpg', shape=content.shape[-2:]).to(device)\n",
        "\n",
        "# Function to un-normalize an image and converting it from tensor image to a numpy image for display\n",
        "def im_convert(tensor):\n",
        "    # Display a tensor as an aimage\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image\n",
        "\n",
        "# display the images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "# Content and style ims side by side\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(style))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# print out vgg19 structure so you can see the names of various layers\n",
        "# print(vgg)\n",
        "\n",
        "def get_features(image, model, layers=None):\n",
        "    # Run an image forward through a model and get features for a set of layers.\n",
        "    # Default layers are for VGGNet matching Gatys\n",
        "    if layers is None:\n",
        "        layers = {\n",
        "            '0': 'conv1_1',\n",
        "            '5': 'conv2_1',\n",
        "            '10': 'conv3_1',\n",
        "            '15': 'conv4_1',\n",
        "            '21': 'conv4_2',\n",
        "            '28': 'conv5_1',\n",
        "        }\n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules is a dictionary holding each module in the model\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    # Calculate teh Gram Matrix of a given tensor\n",
        "    # get teh batch_size, depth, height and width of the Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "\n",
        "    # reshape so we're multiplying teh features for each comment\n",
        "    tensor = tensor.view(d, h*w)\n",
        "\n",
        "    # calculate the gram matrix\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "\n",
        "    return gram\n",
        "\n",
        "# get content and style features only once before tarining\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# Calculate teh gram matrices for each layer of our style representation\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# Create a third target image and prep it for change\n",
        "# it is a good idea to start off with teh target as a copy of our *content* image\n",
        "#then iteratively change its style\n",
        "target = content.clone().requires_grad_(True).to(device)\n",
        "\n",
        "# weights for each style layer\n",
        "# weighting earlier layers more will result in *larger* style artifacts\n",
        "# notice we are excluding `conv4_2` our content representation\n",
        "stye_weights = {\n",
        "    'conv1_1' : 1.,\n",
        "    'conv2_1' : 0.75,\n",
        "    'conv3_1' : 0.2,\n",
        "    'conv4_1' : 0.2,\n",
        "    'conv5_1' : 0.2,\n",
        "}\n",
        "\n",
        "content_weight = 1 # alpha\n",
        "style_weight = 1e6 # beta\n",
        "\n",
        "# for displaying the target image, intermittently\n",
        "show_every = 400\n",
        "\n",
        "# iteration hyperparameters\n",
        "optimizer = optim.Adam([target], lr=0.003)\n",
        "steps = 2000\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "    # get the features from your target image\n",
        "    target_features = get_features(target, vgg)\n",
        "\n",
        "    # Content loss\n",
        "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "\n",
        "    # the style loss\n",
        "    # intialize teh style loss to 0\n",
        "    style_loss = 0\n",
        "    # then add to it for each layer's gram matrix loss\n",
        "    for layer in stye_weights:\n",
        "        # get teh \"target\" style representation for the layer\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "\n",
        "        _, d, h, w = target_feature.shape\n",
        "        # get teh \"style\" style representation\n",
        "        style_gram = style_grams[layer]\n",
        "        # teh style loss for one layer, weighted appropriately\n",
        "        layer_style_loss = stye_weights[layer]*torch.mean(target_gram - style_gram)**2\n",
        "\n",
        "        # add to the style loss\n",
        "        style_loss += layer_style_loss / (d * h *w)\n",
        "\n",
        "    # calculate teh *total* loss\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "    # update your target image\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # display intermediate images and print the loss\n",
        "    if ii % show_every == 0:\n",
        "        print(\"Total loss: \", total_loss.item())\n",
        "\n",
        "        # plt.imshow(im_convert(target))\n",
        "        # plt.show()\n",
        "\n",
        "# display content and final, target image\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 574673361/574673361 [00:08<00:00, 64884452.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9fc9fff65d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# load in content and style image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tiger.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m# Resize style to match content, makes code easier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fire.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e9fc9fff65d6>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(img_path, max_size, shape)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Large images will slow down processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiger.jpeg'"
          ]
        }
      ]
    }
  ]
}